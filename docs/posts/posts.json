[
  {
    "path": "posts/2021-05-23-Article/",
    "title": "Short Article",
    "description": "Article title will go here.",
    "author": [
      {
        "name": "Emma Fahey",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [],
    "contents": "\r\nIntroduction\r\nMethod\r\n\r\n\r\n# select the variables needed \r\ndf2 <- df %>%\r\n  select(gender_code, age_clean, swl_1, swl_2, swl_3, swl_4, swl_5, swls, shs_1_1, shs_2_1, shs_3_1, shs_4_1, shs, motsad, mothap, motref)\r\nhead(df2)\r\n\r\n\r\n# A tibble: 6 x 16\r\n  gender_code age_clean swl_1 swl_2 swl_3 swl_4 swl_5  swls shs_1_1\r\n  <chr>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\r\n1 female             18     4     5     5     4     5   4.6       5\r\n2 female             19     4     5     5     5     7   5.2       6\r\n3 male               20     5     5     6     5     2   4.6       5\r\n4 female             18     2     5     4     3     4   3.6       5\r\n5 female             18     6     6     5     6     7   6         5\r\n6 female             24     5     4     6     3     3   4.2       4\r\n# ... with 7 more variables: shs_2_1 <dbl>, shs_3_1 <dbl>,\r\n#   shs_4_1 <dbl>, shs <dbl>, motsad <dbl>, mothap <dbl>,\r\n#   motref <dbl>\r\n\r\n# table - code for descriptive stats of the scales (means etc.) \r\ntable1::label(df$swls) <- \"Life Satisfaction\"\r\ntable1::label(df$shs) <- \"Happiness\"\r\ntable1::label(df$mothap) <- \"Happy Motivation\"\r\ntable1::label(df$motsad) <- \"Sad Motivation\"\r\ntable1::label(df$motref) <- \"Reflective Motivation\"\r\n\r\n# actual table \r\ntable1::table1(~ swls + shs + mothap + motsad + motref, data = df2)\r\n\r\n\r\n\r\nOverall(N=303)\r\nswls\r\n\r\nMean (SD)\r\n4.49 (1.27)\r\nMedian [Min, Max]\r\n4.60 [1.20, 7.00]\r\nMissing\r\n1 (0.3%)\r\nshs\r\n\r\nMean (SD)\r\n4.27 (0.759)\r\nMedian [Min, Max]\r\n4.25 [1.25, 6.50]\r\nMissing\r\n1 (0.3%)\r\nmothap\r\n\r\nMean (SD)\r\n5.13 (1.56)\r\nMedian [Min, Max]\r\n5.50 [1.00, 7.00]\r\nMissing\r\n1 (0.3%)\r\nmotsad\r\n\r\nMean (SD)\r\n4.04 (1.69)\r\nMedian [Min, Max]\r\n4.14 [1.00, 7.00]\r\nMissing\r\n1 (0.3%)\r\nmotref\r\n\r\nMean (SD)\r\n4.73 (1.82)\r\nMedian [Min, Max]\r\n5.00 [1.00, 7.00]\r\nMissing\r\n1 (0.3%)\r\n\r\n\r\n# this code is used to create an apa table with the below code. \r\n\r\n\r\n\r\n\r\n\r\n\r\n# histograms \r\n# desriptive graphs to show distribution of the data \r\np1 <-\r\n  qplot(df$swls, geom = \"histogram\") + labs(title = \"Life Satisfaction\") + xlab(\"swls\")\r\np2 <-\r\n  qplot(df$shs, geom = \"histogram\") + labs(title = \"Happiness\") + xlab(\"shs\")\r\np3 <- \r\n  qplot(df$mothap, geom = \"histogram\") + labs(title = \"Happy Motivation\") + xlab(\"mothap\")\r\np4 <- \r\n  qplot(df$motsad, geom = \"histogram\") + labs(title = \"Sad Motivation\") + xlab(\"motsad\")\r\np5 <- \r\n  qplot(df$motref, geom = \"histogram\") + labs(title = \"Reflective Motivation\") + xlab(\"motref\")\r\n\r\np1 + p2 + p3 + p4 + p5\r\n\r\n\r\n\r\n# concern is that the 3x IVs are not normally distributed as below... Do I need to adjust the model that I use due to this? \r\n\r\n# Can I do a DAG here or not appropriate? \r\n\r\n\r\n\r\n\r\n\r\nprint(psych::alpha(df2[paste0(\"swl_\", 1:5)]))\r\n\r\n\r\n\r\nReliability analysis   \r\nCall: psych::alpha(x = df2[paste0(\"swl_\", 1:5)])\r\n\r\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\r\n      0.84      0.85    0.83      0.52 5.5 0.015  4.5 1.3     0.52\r\n\r\n lower alpha upper     95% confidence boundaries\r\n0.81 0.84 0.87 \r\n\r\n Reliability if an item is dropped:\r\n      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\r\nswl_1      0.79      0.79    0.76      0.49 3.8    0.020 0.0085  0.52\r\nswl_2      0.81      0.81    0.77      0.52 4.4    0.018 0.0059  0.52\r\nswl_3      0.78      0.79    0.76      0.49 3.8    0.020 0.0107  0.49\r\nswl_4      0.82      0.82    0.79      0.54 4.6    0.018 0.0149  0.56\r\nswl_5      0.85      0.85    0.81      0.58 5.5    0.014 0.0039  0.57\r\n\r\n Item statistics \r\n        n raw.r std.r r.cor r.drop mean  sd\r\nswl_1 302  0.84  0.84  0.80   0.73  4.3 1.7\r\nswl_2 302  0.78  0.79  0.72   0.65  5.0 1.6\r\nswl_3 302  0.84  0.84  0.80   0.74  4.7 1.5\r\nswl_4 302  0.75  0.76  0.67   0.62  4.7 1.5\r\nswl_5 302  0.72  0.70  0.58   0.53  3.9 1.9\r\n\r\nNon missing response frequency for each item\r\n         1    2    3    4    5    6    7 miss\r\nswl_1 0.06 0.12 0.17 0.13 0.28 0.18 0.07    0\r\nswl_2 0.03 0.06 0.10 0.13 0.23 0.32 0.14    0\r\nswl_3 0.04 0.08 0.12 0.14 0.28 0.27 0.08    0\r\nswl_4 0.03 0.06 0.14 0.15 0.29 0.24 0.09    0\r\nswl_5 0.12 0.17 0.17 0.10 0.19 0.18 0.08    0\r\n\r\nprint(psych::alpha(df2[paste0(\"shs_\", 1:4, \"_1\")]))\r\n\r\n\r\nSome items ( shs_4_1 ) were negatively correlated with the total scale and \r\nprobably should be reversed.  \r\nTo do this, run the function again with the 'check.keys=TRUE' option\r\nReliability analysis   \r\nCall: psych::alpha(x = df2[paste0(\"shs_\", 1:4, \"_1\")])\r\n\r\n  raw_alpha std.alpha G6(smc) average_r  S/N   ase mean   sd median_r\r\n    -0.059      0.12    0.59     0.031 0.13 0.087  4.3 0.76    0.035\r\n\r\n lower alpha upper     95% confidence boundaries\r\n-0.23 -0.06 0.11 \r\n\r\n Reliability if an item is dropped:\r\n        raw_alpha std.alpha G6(smc) average_r   S/N alpha se  var.r\r\nshs_1_1     -1.14     -0.91    0.15     -0.19 -0.48    0.208 0.5107\r\nshs_2_1     -1.34     -0.97    0.17     -0.20 -0.49    0.230 0.5518\r\nshs_3_1     -1.03     -0.68    0.28     -0.16 -0.41    0.207 0.5614\r\nshs_4_1      0.85      0.86    0.80      0.67  6.04    0.014 0.0014\r\n        med.r\r\nshs_1_1 -0.56\r\nshs_2_1 -0.61\r\nshs_3_1 -0.56\r\nshs_4_1  0.66\r\n\r\n Item statistics \r\n          n raw.r std.r r.cor r.drop mean  sd\r\nshs_1_1 302  0.80  0.84  0.85   0.51  4.8 1.4\r\nshs_2_1 302  0.83  0.85  0.83   0.52  4.4 1.5\r\nshs_3_1 302  0.78  0.79  0.76   0.37  4.0 1.6\r\nshs_4_1 302 -0.33 -0.39 -0.83  -0.69  3.9 1.7\r\n\r\nNon missing response frequency for each item\r\n           1    2    3    4    5    6    7 miss\r\nshs_1_1 0.01 0.08 0.07 0.16 0.33 0.29 0.05    0\r\nshs_2_1 0.03 0.11 0.14 0.22 0.24 0.21 0.06    0\r\nshs_3_1 0.08 0.14 0.18 0.16 0.26 0.16 0.03    0\r\nshs_4_1 0.06 0.21 0.19 0.14 0.21 0.12 0.08    0\r\n\r\nprint(psych::alpha(df[paste0(\"importance_\", 1:7)]))\r\n\r\n\r\n\r\nReliability analysis   \r\nCall: psych::alpha(x = df[paste0(\"importance_\", 1:7)])\r\n\r\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\r\n      0.94      0.94    0.94      0.68  15 0.0057    4 1.7     0.67\r\n\r\n lower alpha upper     95% confidence boundaries\r\n0.92 0.94 0.95 \r\n\r\n Reliability if an item is dropped:\r\n             raw_alpha std.alpha G6(smc) average_r S/N alpha se\r\nimportance_1      0.92      0.92    0.91      0.66  12   0.0071\r\nimportance_2      0.92      0.92    0.92      0.67  12   0.0069\r\nimportance_3      0.93      0.93    0.93      0.69  13   0.0064\r\nimportance_4      0.93      0.93    0.93      0.68  13   0.0066\r\nimportance_5      0.92      0.92    0.92      0.67  12   0.0070\r\nimportance_6      0.93      0.93    0.93      0.68  13   0.0067\r\nimportance_7      0.93      0.93    0.93      0.69  14   0.0063\r\n              var.r med.r\r\nimportance_1 0.0025  0.66\r\nimportance_2 0.0023  0.67\r\nimportance_3 0.0048  0.67\r\nimportance_4 0.0061  0.67\r\nimportance_5 0.0055  0.66\r\nimportance_6 0.0059  0.66\r\nimportance_7 0.0047  0.67\r\n\r\n Item statistics \r\n               n raw.r std.r r.cor r.drop mean  sd\r\nimportance_1 302  0.89  0.89  0.89   0.85  4.6 1.8\r\nimportance_2 302  0.87  0.88  0.87   0.82  4.5 2.0\r\nimportance_3 302  0.82  0.82  0.78   0.75  3.8 2.1\r\nimportance_4 302  0.84  0.84  0.81   0.78  3.8 2.0\r\nimportance_5 302  0.87  0.87  0.85   0.82  4.1 2.0\r\nimportance_6 302  0.85  0.85  0.81   0.79  3.8 2.0\r\nimportance_7 302  0.81  0.81  0.76   0.73  3.7 2.0\r\n\r\nNon missing response frequency for each item\r\n                1    2    3    4    5    6    7 miss\r\nimportance_1 0.10 0.06 0.10 0.13 0.23 0.23 0.15    0\r\nimportance_2 0.13 0.09 0.09 0.10 0.21 0.23 0.17    0\r\nimportance_3 0.19 0.16 0.12 0.12 0.15 0.14 0.12    0\r\nimportance_4 0.19 0.16 0.13 0.13 0.13 0.15 0.12    0\r\nimportance_5 0.15 0.10 0.14 0.15 0.17 0.18 0.12    0\r\nimportance_6 0.19 0.14 0.17 0.12 0.13 0.14 0.13    0\r\nimportance_7 0.19 0.17 0.14 0.11 0.15 0.13 0.11    0\r\n\r\nprint(psych::alpha(df[paste0(\"importance_\", 8:11)]))\r\n\r\n\r\n\r\nReliability analysis   \r\nCall: psych::alpha(x = df[paste0(\"importance_\", 8:11)])\r\n\r\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\r\n       0.9      0.91    0.89      0.71 9.8 0.0098  5.1 1.6     0.71\r\n\r\n lower alpha upper     95% confidence boundaries\r\n0.88 0.9 0.92 \r\n\r\n Reliability if an item is dropped:\r\n              raw_alpha std.alpha G6(smc) average_r  S/N alpha se\r\nimportance_8       0.86      0.87    0.84      0.69  6.7   0.0143\r\nimportance_9       0.85      0.86    0.83      0.67  6.1   0.0159\r\nimportance_10      0.84      0.85    0.81      0.66  5.8   0.0160\r\nimportance_11      0.93      0.93    0.90      0.82 13.9   0.0068\r\n                var.r med.r\r\nimportance_8  0.01560  0.62\r\nimportance_9  0.02182  0.62\r\nimportance_10 0.01470  0.62\r\nimportance_11 0.00064  0.83\r\n\r\n Item statistics \r\n                n raw.r std.r r.cor r.drop mean  sd\r\nimportance_8  302  0.89  0.90  0.87   0.81  5.5 1.6\r\nimportance_9  302  0.91  0.92  0.89   0.84  5.2 1.8\r\nimportance_10 302  0.92  0.93  0.92   0.86  5.4 1.7\r\nimportance_11 302  0.81  0.79  0.66   0.64  4.5 2.0\r\n\r\nNon missing response frequency for each item\r\n                 1    2    3    4    5    6    7 miss\r\nimportance_8  0.04 0.04 0.05 0.08 0.19 0.28 0.32    0\r\nimportance_9  0.06 0.06 0.06 0.09 0.16 0.31 0.26    0\r\nimportance_10 0.03 0.06 0.07 0.09 0.15 0.31 0.29    0\r\nimportance_11 0.11 0.11 0.10 0.14 0.13 0.20 0.20    0\r\n\r\nprint(psych::alpha(df[paste0(\"importance_\", 12:14)]))\r\n\r\n\r\n\r\nReliability analysis   \r\nCall: psych::alpha(x = df[paste0(\"importance_\", 12:14)])\r\n\r\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\r\n      0.82      0.83    0.77      0.62 4.8 0.018  4.7 1.8     0.58\r\n\r\n lower alpha upper     95% confidence boundaries\r\n0.79 0.82 0.86 \r\n\r\n Reliability if an item is dropped:\r\n              raw_alpha std.alpha G6(smc) average_r S/N alpha se\r\nimportance_12      0.82      0.82    0.70      0.70 4.6    0.021\r\nimportance_13      0.72      0.72    0.56      0.56 2.6    0.032\r\nimportance_14      0.73      0.74    0.58      0.58 2.8    0.030\r\n              var.r med.r\r\nimportance_12    NA  0.70\r\nimportance_13    NA  0.56\r\nimportance_14    NA  0.58\r\n\r\n Item statistics \r\n                n raw.r std.r r.cor r.drop mean  sd\r\nimportance_12 302  0.84  0.83  0.68   0.62  4.6 2.3\r\nimportance_13 302  0.87  0.88  0.80   0.72  5.3 1.9\r\nimportance_14 302  0.88  0.87  0.79   0.70  4.3 2.2\r\n\r\nNon missing response frequency for each item\r\n                 1    2    3    4    5    6    7 miss\r\nimportance_12 0.18 0.08 0.06 0.09 0.11 0.18 0.29    0\r\nimportance_13 0.07 0.06 0.04 0.10 0.12 0.25 0.36    0\r\nimportance_14 0.19 0.08 0.08 0.13 0.14 0.17 0.21    0\r\n\r\n# i think there something wrong with the reverse coding of shs.... see reliability output... How do I fix this? \r\n\r\n# other reliability \r\n\r\nprint(ufs::scaleStructure(df2[paste0(\"swl_\", 1:5)]))\r\n\r\n\r\n\r\nInformation about this analysis:\r\n\r\n                 Dataframe: df2[paste0(\"swl_\", 1:5)]\r\n                     Items: all\r\n              Observations: 302\r\n     Positive correlations: 10 out of 10 (100%)\r\n\r\nEstimates assuming interval level:\r\n\r\n             Omega (total): 0.88\r\n      Omega (hierarchical): 0.79\r\n   Revelle's omega (total): 0.88\r\nGreatest Lower Bound (GLB): 0.87\r\n             Coefficient H: 0.87\r\n         Coefficient alpha: 0.84\r\n\r\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\r\n\r\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information.\r\n\r\nprint(ufs::scaleStructure(df2[paste0(\"shs_\", 1:4, \"_1\")]))\r\n\r\n\r\n\r\nInformation about this analysis:\r\n\r\n                 Dataframe: df2[paste0(\"shs_\", 1:4, \"_1\")]\r\n                     Items: all\r\n              Observations: 302\r\n     Positive correlations: 3 out of 6 (50%)\r\n\r\nEstimates assuming interval level:\r\n\r\n             Omega (total): 0.89\r\n      Omega (hierarchical): 0.85\r\n   Revelle's omega (total): 0.89\r\nGreatest Lower Bound (GLB): 0.73\r\n             Coefficient H: 0.88\r\n         Coefficient alpha: -0.06\r\n\r\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\r\n\r\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information.\r\n\r\nprint(ufs::scaleStructure(df[paste0(\"importance_\", 1:7)]))\r\n\r\n\r\n\r\nInformation about this analysis:\r\n\r\n                 Dataframe: df[paste0(\"importance_\", 1:7)]\r\n                     Items: all\r\n              Observations: 302\r\n     Positive correlations: 21 out of 21 (100%)\r\n\r\nEstimates assuming interval level:\r\n\r\n             Omega (total): 0.96\r\n      Omega (hierarchical): 0.84\r\n   Revelle's omega (total): 0.96\r\nGreatest Lower Bound (GLB): 0.96\r\n             Coefficient H: 0.95\r\n         Coefficient alpha: 0.94\r\n\r\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\r\n\r\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information.\r\n\r\nprint(ufs::scaleStructure(df[paste0(\"importance_\", 8:11)]))\r\n\r\n\r\n\r\nInformation about this analysis:\r\n\r\n                 Dataframe: df[paste0(\"importance_\", 8:11)]\r\n                     Items: all\r\n              Observations: 302\r\n     Positive correlations: 6 out of 6 (100%)\r\n\r\nEstimates assuming interval level:\r\n\r\n             Omega (total): 0.92\r\n      Omega (hierarchical): 0.05\r\n   Revelle's omega (total): 0.92\r\nGreatest Lower Bound (GLB): 0.93\r\n             Coefficient H: 0.94\r\n         Coefficient alpha: 0.9\r\n\r\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\r\n\r\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information.\r\n\r\nprint(ufs::scaleStructure(df[paste0(\"importance_\", 12:14)]))\r\n\r\n\r\n\r\nInformation about this analysis:\r\n\r\n                 Dataframe: df[paste0(\"importance_\", 12:14)]\r\n                     Items: all\r\n              Observations: 302\r\n     Positive correlations: 3 out of 3 (100%)\r\n\r\nEstimates assuming interval level:\r\n\r\n             Omega (total): 0.83\r\n      Omega (hierarchical): 0.08\r\n   Revelle's omega (total): 0.83\r\nGreatest Lower Bound (GLB): 0.84\r\n             Coefficient H: 0.85\r\n         Coefficient alpha: 0.82\r\n\r\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\r\n\r\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information.\r\n\r\n# reliability graph \r\n\r\n# can't do - error \r\n# what is EBICglasso, should I be changing this? \r\n\r\n\r\n\r\n\r\n\r\n# SEM \r\n\r\nmdl_sem <- 'swls ~ motsad + mothap + motref\r\n  shs ~ motsad + mothap + motref'\r\n  \r\nsem_res <- sem(mdl_sem, df2) \r\nsummary(sem_res, standardize = T)\r\n\r\n\r\nlavaan 0.6-8 ended normally after 19 iterations\r\n\r\n  Estimator                                         ML\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                         9\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                           302         303\r\n                                                                  \r\nModel Test User Model:\r\n                                                      \r\n  Test statistic                                 0.000\r\n  Degrees of freedom                                 0\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                             Standard\r\n  Information                                 Expected\r\n  Information saturated (h1) model          Structured\r\n\r\nRegressions:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n  swls ~                                                       \r\n    motsad           -0.067    0.059   -1.151    0.250   -0.067\r\n    mothap            0.104    0.053    1.977    0.048    0.104\r\n    motref            0.043    0.053    0.814    0.416    0.043\r\n  shs ~                                                        \r\n    motsad           -0.042    0.034   -1.218    0.223   -0.042\r\n    mothap            0.109    0.031    3.573    0.000    0.109\r\n    motref            0.047    0.031    1.521    0.128    0.047\r\n  Std.all\r\n         \r\n   -0.090\r\n    0.127\r\n    0.061\r\n         \r\n   -0.093\r\n    0.225\r\n    0.112\r\n\r\nCovariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n .swls ~~                                                      \r\n   .shs               0.465    0.060    7.786    0.000    0.465\r\n  Std.all\r\n         \r\n    0.501\r\n\r\nVariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n   .swls              1.591    0.129   12.288    0.000    1.591\r\n   .shs               0.540    0.044   12.288    0.000    0.540\r\n  Std.all\r\n    0.983\r\n    0.941\r\n\r\nsem_res_strap <- sem(mdl_sem, df, se = \"bootstrap\") \r\nsummary(sem_res_strap, standardize = T) \r\n\r\n\r\nlavaan 0.6-8 ended normally after 19 iterations\r\n\r\n  Estimator                                         ML\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                         9\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                           302         303\r\n                                                                  \r\nModel Test User Model:\r\n                                                      \r\n  Test statistic                                 0.000\r\n  Degrees of freedom                                 0\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                            Bootstrap\r\n  Number of requested bootstrap draws             1000\r\n  Number of successful bootstrap draws            1000\r\n\r\nRegressions:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n  swls ~                                                       \r\n    motsad           -0.067    0.064   -1.057    0.291   -0.067\r\n    mothap            0.104    0.058    1.786    0.074    0.104\r\n    motref            0.043    0.058    0.742    0.458    0.043\r\n  shs ~                                                        \r\n    motsad           -0.042    0.034   -1.240    0.215   -0.042\r\n    mothap            0.109    0.029    3.711    0.000    0.109\r\n    motref            0.047    0.031    1.527    0.127    0.047\r\n  Std.all\r\n         \r\n   -0.090\r\n    0.127\r\n    0.061\r\n         \r\n   -0.093\r\n    0.225\r\n    0.112\r\n\r\nCovariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n .swls ~~                                                      \r\n   .shs               0.465    0.055    8.391    0.000    0.465\r\n  Std.all\r\n         \r\n    0.501\r\n\r\nVariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n   .swls              1.591    0.128   12.466    0.000    1.591\r\n   .shs               0.540    0.047   11.392    0.000    0.540\r\n  Std.all\r\n    0.983\r\n    0.941\r\n\r\n# Bootstrapping randomly grabs part of your data and makes a new average. Allows confidence intervals. \r\n# IV – 3 diff motivations X\r\n# DV – life sat and happy scores Y\r\n\r\n# next steps \r\n# how do i graph or visually present this? Can I dag it? How do i report it? \r\n# also what does it all mean? \r\n\r\n\r\n\r\nText here\r\nIn chunk 2 I think - this code errors.\r\nnetwork1 <- bootnet::estimateNetwork(select(df2, -age_clean, -gender_code), “EBICglasso”) qgraph::qgraph(network1$graph, layout = “spring”)\r\nOther Code we have:\r\nCreate table data for apa format\r\ntab_01 = data.frame( scale = c(“BAS-T”, “SR”, “BDI”, “ASRM”, “M-SRM”), high = c(“46.17 (2.87)”, “17.94 (1.88)”, “7.11 (6.50)”, “6.46 (4.01)”, “11.05 (3.36)”), moderate = c(“37.99 (1.32)”, “11.52 (1.84)”, “6.18 (6.09)”, “5.63 (3.69)”, “11.76 (2.75)”), p = c(“<.001”, “<.001”, “.254”, “.109”, “.078”) )\r\napa format of a table\r\nkable( tab_01, format = “latex”, booktabs = TRUE, escape = FALSE, longtable = TRUE, col.names = c(“Scale”, “High BAS group”, “Moderate BAS group”, “\\textit{p}”), align = c(“l”, “c”, “c”, “c”), caption = “Means and Standard Deviations of Scores on Baseline Measures” ) %>% row_spec(row = 0, align = “c”) %>% kable_styling(full_width = TRUE) %>% footnote( general_title = “Note.”, general = “Standard deviations are presented in parentheses. BAS = Behavioral Activation System; BAS-T = Behavioral Activation System-Total sores from the Behavioral Inhibition System/Behavioral Activation System Scales; SR = Sensitivity to Reward scores from the Sensitivity to Punishment and Sensitivity to Reward Questionnaire; BDI = Beck Depression Inventory scores; ASRM = Altman Self-Rating Mania Scale scores; M-SRM = Modified Social Rhythm Metric Regularity scores.”, threeparttable = TRUE, footnote_as_chunk = TRUE )\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-23-Article/Fahey.E-Article_files/figure-html5/descriptives-1.png",
    "last_modified": "2021-05-26T15:34:20+12:00",
    "input_file": "Fahey.E-Article.knit.md"
  },
  {
    "path": "posts/2021-05-19-blog2/",
    "title": "DAGs",
    "description": "Workbook 11 for the fulfillment of PSYC447 course (VUW).",
    "author": [
      {
        "name": "Emma Fahey",
        "url": {}
      }
    ],
    "date": "2021-05-19",
    "categories": [],
    "contents": "\r\nThree questions will be answered in this lab\r\nWhat is the difference between prediction and causal inference: give an example of how regression may be use useful for each task. Make sure your explanation includes a DAG.\r\n\r\n\r\n# this code chunk runs a successful dag. \r\n\r\n# data is nz_cr\r\n\r\n# y = life meaning LIFEMEANING\r\n# x = Religiousity Relid\r\n# z = Agreeableness AGREEABLENESS\r\n\r\n# use this code in the main methods section of report (after descriptive stats.)\r\n\r\n# code for DAG \r\nggdag_1 <- dagify(LIFEMEANING ~ AGREEABLENESS,\r\n                       Relid ~ AGREEABLENESS,\r\n                       exposure = \"Relid\",\r\n                       outcome = \"LIFEMEANING\") %>%\r\n  tidy_dagitty(layout = \"tree\")\r\n\r\n# plot the DAG\r\nggdag_1 %>%\r\n  ggdag() + \r\n  theme_dag_blank()\r\n\r\n\r\n\r\n# then we ask ggdag which variables we need to include if we are to obtain an unbiased estimate of the outcome from the exposure:\r\n\r\nggdag::ggdag_adjustment_set( ggdag_1 ) + \r\n  theme_dag_blank()\r\n\r\n\r\n\r\n#The graph tells us to obtain an unbiased estimate of Y on X we must condition on Z (agreeableness). \r\n# And indeed, when we included the omitted variable Z in our simulated dateset it breaks the association between X and Y:\r\n\r\nm1 <- lm(LIFEMEANING ~ Relid + AGREEABLENESS, data = nz_cr)\r\n\r\nparameters::model_parameters(m1)\r\n\r\n\r\nParameter     | Coefficient |       SE |       95% CI | t(4287) |      p\r\n------------------------------------------------------------------------\r\n(Intercept)   |        3.94 |     0.09 | [3.77, 4.12] |   43.15 | < .001\r\nRelid         |        0.06 | 6.32e-03 | [0.04, 0.07] |    8.99 | < .001\r\nAGREEABLENESS |        0.27 |     0.02 | [0.24, 0.30] |   15.91 | < .001\r\n\r\nreport::report(m1)\r\n\r\n\r\nWe fitted a linear model (estimated using OLS) to predict LIFEMEANING with Relid and AGREEABLENESS (formula: LIFEMEANING ~ Relid + AGREEABLENESS). The model explains a statistically significant and weak proportion of variance (R2 = 0.08, F(2, 4287) = 183.65, p < .001, adj. R2 = 0.08). The model's intercept, corresponding to Relid = 0 and AGREEABLENESS = 0, is at 3.94 (95% CI [3.77, 4.12], t(4287) = 43.15, p < .001). Within this model:\r\n\r\n  - The effect of Relid is statistically significant and positive (beta = 0.06, 95% CI [0.04, 0.07], t(4287) = 8.99, p < .001; Std. beta = 0.13, 95% CI [0.10, 0.16])\r\n  - The effect of AGREEABLENESS is statistically significant and positive (beta = 0.27, 95% CI [0.24, 0.30], t(4287) = 15.91, p < .001; Std. beta = 0.23, 95% CI [0.21, 0.26])\r\n\r\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\r\n\r\nExplain “collider confounding” (or “collider bias”), and explain how collider bias can spoil inference. Make sure that your explanation includes a DAG.\r\n\r\n\r\n# collider code explanation \r\n# Imagine in science there is no relationship between the newsworthiness (N) of science and its trustworthiness (T). Imagine further that selection (S) committees make decisions on the basis of the both newsworthiness and the trustworthiness of scientific proposals.\r\n\r\n# collider code \r\n\r\n# s could be employment, gender, something 0 or 1. \r\n# height and religiousness on employment  \r\n\r\n# n = height (HLTH.Height)\r\n# T = religiousity (Relid) \r\n# s = employment (Employed)\r\n\r\ndag_sd <- dagify(Employed ~ HLTH.Height,\r\n                 Employed ~ Relid,\r\n                 labels = c(\"Employed\" = \"Employed\",\r\n                            \"HLTH.Height\" = \"Height\",\r\n                            \"Relid\" = \"Religiousity\")) %>%\r\n  tidy_dagitty(layout = \"nicely\")\r\n\r\n# Graph\r\ndag_sd %>%\r\n  ggdag(text = FALSE, use_labels = \"label\") + \r\n  theme_dag_blank()\r\n\r\n\r\n\r\n# next section \r\nggdag_dseparated(\r\n  dag_sd,\r\n  from = \"Relid\",\r\n  to = \"HLTH.Height\",\r\n  controlling_for = \"Employed\",\r\n  text = FALSE,\r\n  use_labels = \"label\", \r\n) + \r\n  theme_dag_blank()\r\n\r\n\r\n\r\n# code for finding colliders\r\nggdag::ggdag_collider(dag_sd,\r\n                      text = FALSE,\r\n                      use_labels = \"label\") + \r\n  theme_dag_blank()\r\n\r\n\r\n\r\n# the end - then explain that. \r\n\r\n\r\n\r\nUsing the nzl dataset, select at least five demongraphic/ideological variables that might be related to an exposure variable and and outcome variable of your choice. Create a DAG and identify which variables you should include to obtain an unbiased estimate of the causal effect of your exposure variable on your outcome variable. Test your model and interpret the results.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-19-blog2/Blog_2_files/figure-html5/question 1-1.png",
    "last_modified": "2021-05-19T17:25:22+12:00",
    "input_file": "Blog_2.knit.md"
  },
  {
    "path": "posts/2021-05-12-welcome/",
    "title": "An Example of a Varying Slope Model",
    "description": "Workbook 10 for the fulfillment of 447 course (VUW).",
    "author": [
      {
        "name": "Emma Fahey",
        "url": {}
      }
    ],
    "date": "2021-05-12",
    "categories": [],
    "contents": "\r\nWeek 10 Workbook\r\nUsing the nzl dataset or another dataset of your choice, write a varying intercept/varying slope model.\r\nThe variables I have identified to answer this question are Personal Relationships and Agreeableness. This is due to the potential association between the variables over the years (waves). I hypothesize that as agreeableness increase, personal relationships should also increase. I expect these variables to change slightly over years as people develop and grow.\r\nBrief Methods\r\nThe data for this short report is taken from the New Zealand Attitudes and Values over time dataset (NZAVS). This dataset includes 2845 individuals that have responded on at least five different years to both personal relationship and agreeableness questions.\r\nWe fit the variables to a varying slope model to try and determine if the relationship between agreeableness and personal relationships changes over time. Each slope represents a different wave of the data (or different year that data was collected).\r\nFinally, we fit the model to a conditional effects graph to check that the two variables do indeed associate with each other in some way.\r\n\r\n\r\n \r\n\r\n\r\nAGREEABLENESS\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI (95%)\r\n\r\n\r\nIntercept\r\n\r\n\r\n4.82\r\n\r\n\r\n4.76 – 4.87\r\n\r\n\r\nYour.Personal.Relationships\r\n\r\n\r\n0.07\r\n\r\n\r\n0.06 – 0.08\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n0.89\r\n\r\n\r\nτ00Wave\r\n\r\n0.00\r\n\r\n\r\nτ11Wave.Your.Personal.Relationships\r\n\r\n0.00\r\n\r\n\r\nρ01\r\n\r\n \r\n\r\n\r\nρ01\r\n\r\n \r\n\r\n\r\nICC\r\n\r\n\r\n0.00\r\n\r\n\r\nN Wave\r\n\r\n11\r\n\r\n\r\nObservations\r\n\r\n\r\n22067\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.023 / 0.023\r\n\r\n\r\n\r\nThe above model does not appear to fit the model perfectly, so we then attempted to remove the random slope to see if we could generate a model that fits data better. The below model and graph is the result:\r\n\r\n\r\n \r\n\r\n\r\nAGREEABLENESS\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI (95%)\r\n\r\n\r\nIntercept\r\n\r\n\r\n4.82\r\n\r\n\r\n4.77 – 4.87\r\n\r\n\r\nYour.Personal.Relationships\r\n\r\n\r\n0.07\r\n\r\n\r\n0.06 – 0.07\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n0.89\r\n\r\n\r\nτ00Wave\r\n\r\n0.00\r\n\r\n\r\nICC\r\n\r\n\r\n0.00\r\n\r\n\r\nN Wave\r\n\r\n11\r\n\r\n\r\nObservations\r\n\r\n\r\n22067\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.023 / 0.023\r\n\r\n\r\n\r\nDiscussion\r\nBoth models above do not appear to fit the data particularly well and we can interpret that the models explanatory power is weak (R2 = 0.02, 89% CI [0.02, 0.03], adj. R2 = 0.02).\r\nFrom the conditional effects graph above, however, we can see that there may be a relationship between the two variables as this graph shows that as personal relationships increase, so do does agreeableness ratings or scores. Perhaps this relationship is simply consistent over time, which is why no large difference was found over the years in the varying slope models.\r\nThe other limitation of the models fit could be due to the dataset containing the same individuals over several years. Perhaps with different populations, ethnicities and/or countries measures there may be a more varying degree in the way these two variables associate.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-17T17:29:15+12:00",
    "input_file": {}
  }
]
